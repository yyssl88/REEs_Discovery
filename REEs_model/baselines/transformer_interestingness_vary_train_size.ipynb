{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "datasetName = sys.argv[1] #'airports' # 'hospital', 'ncvoter', 'inspection'\n",
    "result_file = sys.argv[2]\n",
    "cudaOption = sys.argv[3]\n",
    "train_round = sys.argv[4]\n",
    "train_ratio = sys.argv[5]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_prefix = '../../REEs_model_data/revision/labeled_data_400/'\n",
    "# option = 'train'\n",
    "# rule_pairs_train_file = os.path.join(data_prefix, datasetName, datasetName + '_rule_pairs_' + option + '.csv')\n",
    "# option = 'valid'\n",
    "# rule_pairs_valid_file = os.path.join(data_prefix, datasetName, datasetName + '_rule_pairs_' + option + '.csv')\n",
    "# option = 'test'\n",
    "# rule_pairs_test_file = os.path.join(data_prefix, datasetName, datasetName + '_rule_pairs_' + option + '.csv')\n",
    "\n",
    "saved_model_path = os.path.join(\"./saved_models/\", datasetName + '-transformer-bc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract rules from rule_pair_ids\n",
    "\n",
    "rules_set_path = os.path.join(data_prefix, datasetName, 'train', 'rules.txt')\n",
    "\n",
    "rules_set = pd.read_csv(rules_set_path)\n",
    "rules_set = rules_set.values\n",
    "\n",
    "for option in ('train_' + train_ratio, 'valid', 'test'):\n",
    "    rule_pair_set_path = os.path.join(data_prefix, datasetName, 'train_' + train_round, option + '.csv')\n",
    "    rules_pair_ids_set = pd.read_csv(rule_pair_set_path, delimiter=\",\")\n",
    "    #rules_pair_ids_set\n",
    "    # split training, validation and testing\n",
    "    # prepare the rule pairs\n",
    "    rules_pairs_set = []\n",
    "    for left_id, right_id, label in rules_pair_ids_set.values:\n",
    "        rule_left, rule_right = rules_set[left_id][0], rules_set[right_id][0]\n",
    "        rules_pairs_set.append([rule_left, rule_right, label])\n",
    "    rules_pairs_set = np.array(rules_pairs_set)\n",
    "    #rules_pairs_set[:10]\n",
    "    # save csv\n",
    "    rules_pairs_df = pd.DataFrame(rules_pairs_set, columns=['rule_left', 'rule_right', 'label'])\n",
    "    #output_file = os.path.join(data_prefix, datasetName, datasetName + '_rule_pairs_' + option + '.csv')\n",
    "    output_file = os.path.join(data_prefix, datasetName, 'train_' + train_round, 'rule_pairs_' + option + '.csv')\n",
    "    rules_pairs_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-65988205d4f8e104\n",
      "Reusing dataset csv (/home/user/.cache/huggingface/datasets/csv/default-65988205d4f8e104/0.0.0/e138af468cb14e747fb46a19c787ffcfa5170c821476d20d5304287ce12bbc23)\n",
      "Using custom data configuration default-fcc043272146ee13\n",
      "Reusing dataset csv (/home/user/.cache/huggingface/datasets/csv/default-fcc043272146ee13/0.0.0/e138af468cb14e747fb46a19c787ffcfa5170c821476d20d5304287ce12bbc23)\n",
      "Using custom data configuration default-02868ac1c48c75dc\n",
      "Reusing dataset csv (/home/user/.cache/huggingface/datasets/csv/default-02868ac1c48c75dc/0.0.0/e138af468cb14e747fb46a19c787ffcfa5170c821476d20d5304287ce12bbc23)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset('csv', data_files='./top-k/rules/test_rules.csv')\n",
    "_datasets = []\n",
    "for option in ('train', 'valid', 'test'):\n",
    "    rule_pairs_file = os.path.join(data_prefix, datasetName, 'train_' + train_round, 'rule_pairs_' + option + '.csv')\n",
    "    tmp_dataset = load_dataset('csv', data_files=rule_pairs_file)\n",
    "    _datasets.append(tmp_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = cudaOption #'2'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['rule_left', 'rule_right', 'label'],\n",
       "        num_rows: 479\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['rule_left', 'rule_right', 'label'],\n",
       "        num_rows: 159\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['rule_left', 'rule_right', 'label'],\n",
       "        num_rows: 161\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, valid_dataset, test_dataset = _datasets\n",
    "\n",
    "dataset = train_dataset\n",
    "dataset['validation'] = valid_dataset['train']\n",
    "dataset['test'] = test_dataset['train']\n",
    "# dataset['train'] = dataset['train']\n",
    "dataset\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertConfig\n",
    "batch_size = 16\n",
    "\n",
    "bertConfig = BertConfig()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', config=bertConfig, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/user/.cache/huggingface/datasets/csv/default-65988205d4f8e104/0.0.0/e138af468cb14e747fb46a19c787ffcfa5170c821476d20d5304287ce12bbc23/cache-d1d5836883633c8f.arrow\n",
      "Loading cached processed dataset at /home/user/.cache/huggingface/datasets/csv/default-fcc043272146ee13/0.0.0/e138af468cb14e747fb46a19c787ffcfa5170c821476d20d5304287ce12bbc23/cache-81f0d95398ee0e71.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55af2b087994bd3bece5499d11a763b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rule1_key, rule2_key = 'rule_left', 'rule_right'\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[rule1_key], examples[rule2_key], truncation=True)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.command.config import config\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_label = 2\n",
    "model = AutoModelForSequenceClassification.from_config(config=bertConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "metric = 'accuracy'\n",
    "\n",
    "args = TrainingArguments(output_dir=saved_model_path,\n",
    "                        evaluation_strategy='epoch',\n",
    "                        save_strategy='epoch',\n",
    "                        learning_rate=2e-5,\n",
    "                        per_device_train_batch_size=batch_size,\n",
    "                        per_device_eval_batch_size=batch_size,\n",
    "                        num_train_epochs=20,\n",
    "                        weight_decay=0.01,\n",
    "                        load_best_model_at_end=True,\n",
    "                        metric_for_best_model='accuracy',\n",
    "                        push_to_hub=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    #print(predictions, labels)\n",
    "    precision = metrics.precision_score(labels, predictions)\n",
    "    recall = metrics.recall_score(labels, predictions)\n",
    "    f1 = metrics.f1_score(labels, predictions)\n",
    "    acc = metrics.accuracy_score(labels, predictions)\n",
    "    return {'precision': precision, 'recall': recall, 'f1': f1, 'accuracy': acc}\n",
    "\n",
    "\n",
    "trainer = Trainer(model, args, train_dataset=encoded_dataset['train'], eval_dataset=encoded_dataset['validation'], tokenizer=tokenizer, compute_metrics=compute_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: rule_left, rule_right.\n",
      "***** Running training *****\n",
      "  Num examples = 479\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.688976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.721834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.692037</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>0.525641</td>\n",
       "      <td>0.534591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: rule_left, rule_right.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 159\n",
      "  Batch size = 16\n",
      "/home/user/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./saved_models/airports-transformer-bc/checkpoint-30\n",
      "Configuration saved in ./saved_models/airports-transformer-bc/checkpoint-30/config.json\n",
      "Model weights saved in ./saved_models/airports-transformer-bc/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/airports-transformer-bc/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/airports-transformer-bc/checkpoint-30/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: rule_left, rule_right.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 159\n",
      "  Batch size = 16\n",
      "/home/user/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./saved_models/airports-transformer-bc/checkpoint-60\n",
      "Configuration saved in ./saved_models/airports-transformer-bc/checkpoint-60/config.json\n",
      "Model weights saved in ./saved_models/airports-transformer-bc/checkpoint-60/pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/airports-transformer-bc/checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/airports-transformer-bc/checkpoint-60/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: rule_left, rule_right.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 159\n",
      "  Batch size = 16\n",
      "/home/user/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./saved_models/airports-transformer-bc/checkpoint-90\n",
      "Configuration saved in ./saved_models/airports-transformer-bc/checkpoint-90/config.json\n",
      "Model weights saved in ./saved_models/airports-transformer-bc/checkpoint-90/pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/airports-transformer-bc/checkpoint-90/tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/airports-transformer-bc/checkpoint-90/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: rule_left, rule_right.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 159\n",
      "  Batch size = 16\n",
      "/home/user/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./saved_models/airports-transformer-bc/checkpoint-120\n",
      "Configuration saved in ./saved_models/airports-transformer-bc/checkpoint-120/config.json\n",
      "Model weights saved in ./saved_models/airports-transformer-bc/checkpoint-120/pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/airports-transformer-bc/checkpoint-120/tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/airports-transformer-bc/checkpoint-120/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: rule_left, rule_right.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 159\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./saved_models/airports-transformer-bc/checkpoint-150\n",
      "Configuration saved in ./saved_models/airports-transformer-bc/checkpoint-150/config.json\n",
      "Model weights saved in ./saved_models/airports-transformer-bc/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/airports-transformer-bc/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/airports-transformer-bc/checkpoint-150/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./saved_models/airports-transformer-bc/checkpoint-30 (score: 0.5471698113207547).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.7088270060221354, metrics={'train_runtime': 35.3496, 'train_samples_per_second': 67.752, 'train_steps_per_second': 4.243, 'total_flos': 160856438138040.0, 'train_loss': 0.7088270060221354, 'epoch': 5.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: rule_left, rule_right.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 159\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6889760494232178,\n",
       " 'eval_precision': 0.0,\n",
       " 'eval_recall': 0.0,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_accuracy': 0.5471698113207547,\n",
       " 'eval_runtime': 0.4592,\n",
       " 'eval_samples_per_second': 346.241,\n",
       " 'eval_steps_per_second': 21.776,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: rule_left, rule_right.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 161\n",
      "  Batch size = 16\n",
      "/home/user/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6872183680534363,\n",
       " 'eval_precision': 0.0,\n",
       " 'eval_recall': 0.0,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_accuracy': 0.5590062111801242,\n",
       " 'eval_runtime': 0.4909,\n",
       " 'eval_samples_per_second': 327.963,\n",
       " 'eval_steps_per_second': 22.407,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = trainer.evaluate(encoded_dataset['test'])\n",
    "\n",
    "f = open(result_file, 'w')\n",
    "f.write(str(ll))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_models/airports-transformer-bc\n",
      "Configuration saved in ./saved_models/airports-transformer-bc/config.json\n",
      "Model weights saved in ./saved_models/airports-transformer-bc/pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/airports-transformer-bc/tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/airports-transformer-bc/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "370e093059bd6f173e0ac27bbfc74cc78af2c2ddbc8766d50665443f5906163e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
